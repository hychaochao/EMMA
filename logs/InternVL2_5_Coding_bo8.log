phoenix-srun: job 3880987 queued and waiting for resources
phoenix-srun: job 3880987 has been allocated resources
phoenix-srun: Job 3880987 scheduled successfully!
Current QUOTA_TYPE is [reserved], which means the job has occupied quota in RESERVED_TOTAL under your partition.
Current PHX_PRIORITY is normal

[root] Reading results/test-time-compute/internvl-best-of-8/InternVL2_5_Coding_8.json
[root] Loading dataset mm-reasoning/EMMA-test100, subject: Coding
[root] Loading config
[root] Loading local model /mnt/petrelfs/share_data/quxiaoye/models/InternVL2_5-78B
[transformers_modules.InternVL2_5-78B.configuration_internvl_chat] vision_select_layer: -1
[transformers_modules.InternVL2_5-78B.configuration_internvl_chat] ps_version: v2
[transformers_modules.InternVL2_5-78B.configuration_internvl_chat] min_dynamic_patch: 1
[transformers_modules.InternVL2_5-78B.configuration_internvl_chat] max_dynamic_patch: 12
/mnt/petrelfs/haoyunzhuo/anaconda3/envs/intern/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
[transformers_modules.InternVL2_5-78B.modeling_internvl_chat] num_image_token: 256
[transformers_modules.InternVL2_5-78B.modeling_internvl_chat] ps_version: v2
Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:09<04:56,  9.27s/it]Loading checkpoint shards:   6%|▌         | 2/33 [00:15<03:48,  7.36s/it]Loading checkpoint shards:   9%|▉         | 3/33 [00:21<03:19,  6.65s/it]Loading checkpoint shards:  12%|█▏        | 4/33 [00:26<02:59,  6.20s/it]Loading checkpoint shards:  15%|█▌        | 5/33 [00:32<02:47,  5.99s/it]Loading checkpoint shards:  18%|█▊        | 6/33 [00:38<02:41,  5.97s/it]Loading checkpoint shards:  21%|██        | 7/33 [00:44<02:34,  5.96s/it]Loading checkpoint shards:  24%|██▍       | 8/33 [00:51<02:37,  6.30s/it]Loading checkpoint shards:  27%|██▋       | 9/33 [00:56<02:26,  6.09s/it]Loading checkpoint shards:  30%|███       | 10/33 [01:03<02:21,  6.16s/it]Loading checkpoint shards:  33%|███▎      | 11/33 [01:08<02:11,  5.98s/it]Loading checkpoint shards:  36%|███▋      | 12/33 [01:14<02:03,  5.86s/it]Loading checkpoint shards:  39%|███▉      | 13/33 [01:20<01:56,  5.84s/it]Loading checkpoint shards:  42%|████▏     | 14/33 [01:25<01:50,  5.81s/it]Loading checkpoint shards:  45%|████▌     | 15/33 [01:31<01:42,  5.69s/it]Loading checkpoint shards:  48%|████▊     | 16/33 [01:38<01:44,  6.12s/it]Loading checkpoint shards:  52%|█████▏    | 17/33 [01:43<01:33,  5.86s/it]Loading checkpoint shards:  55%|█████▍    | 18/33 [01:49<01:26,  5.76s/it]Loading checkpoint shards:  58%|█████▊    | 19/33 [01:54<01:19,  5.66s/it]Loading checkpoint shards:  61%|██████    | 20/33 [02:00<01:13,  5.63s/it]Loading checkpoint shards:  64%|██████▎   | 21/33 [02:05<01:06,  5.57s/it]Loading checkpoint shards:  67%|██████▋   | 22/33 [02:11<01:02,  5.68s/it]Loading checkpoint shards:  70%|██████▉   | 23/33 [02:17<00:56,  5.66s/it]Loading checkpoint shards:  73%|███████▎  | 24/33 [02:24<00:54,  6.05s/it]Loading checkpoint shards:  76%|███████▌  | 25/33 [02:29<00:46,  5.83s/it]Loading checkpoint shards:  79%|███████▉  | 26/33 [02:34<00:40,  5.74s/it]Loading checkpoint shards:  82%|████████▏ | 27/33 [02:39<00:33,  5.55s/it]Loading checkpoint shards:  85%|████████▍ | 28/33 [02:45<00:27,  5.49s/it]Loading checkpoint shards:  88%|████████▊ | 29/33 [02:50<00:21,  5.41s/it]Loading checkpoint shards:  91%|█████████ | 30/33 [02:56<00:16,  5.50s/it]Loading checkpoint shards:  94%|█████████▍| 31/33 [03:01<00:10,  5.44s/it]Loading checkpoint shards:  97%|█████████▋| 32/33 [03:06<00:05,  5.22s/it]Loading checkpoint shards: 100%|██████████| 33/33 [03:09<00:00,  4.62s/it]Loading checkpoint shards: 100%|██████████| 33/33 [03:09<00:00,  5.74s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[root] Model loaded!
[root] Starting to generate.....
  0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.
[root] Scoring list:[5, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0]
  1%|          | 1/100 [05:21<8:50:55, 321.77s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.
[root] Scoring list:[0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0]
[root] Error in generating answer for coding_8
[root] 'response_9'
[root] Save results to results/test-time-compute/internvl-best-of-8/InternVL2_5_Coding_8.json
  2%|▏         | 2/100 [12:44<10:41:24, 392.70s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (17396 > 16384). Running this sequence through the model will result in indexing errors
Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.
[root] Scoring list:[0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0]
[root] Save results to results/test-time-compute/internvl-best-of-8/InternVL2_5_Coding_8.json
  3%|▎         | 3/100 [20:04<11:10:16, 414.60s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.
[root] Scoring list:[0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0]
[root] Save results to results/test-time-compute/internvl-best-of-8/InternVL2_5_Coding_8.json
  4%|▍         | 4/100 [25:17<9:58:53, 374.31s/it] Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.
[root] Scoring list:[0, 5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0]
[root] Error in generating answer for coding_24
[root] 'response_10'
[root] Save results to results/test-time-compute/internvl-best-of-8/InternVL2_5_Coding_8.json
  5%|▌         | 5/100 [32:03<10:10:38, 385.66s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.
